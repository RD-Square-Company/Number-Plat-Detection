{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import imutils\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading of vehicle images done...\n"
     ]
    }
   ],
   "source": [
    "#                           START READING FILES FOR VEHICLES AND NON-VEHICLES\n",
    "# Reading car images\n",
    "vehicle_image_arr1 = glob.glob('/media/rd_square/Important/Image_processing/car_data_with_neg/vehicles/*/*.png')\n",
    "\n",
    "# read images, flip images and append into list\n",
    "vehicle_images_list = []    \n",
    "for imagePath in vehicle_image_arr1:\n",
    "    readImage = cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    rgbImage = cv2.resize(rgbImage, (64,64))\n",
    "    flippedImage = cv2.flip(rgbImage, flipCode=1) # flip horizontally\n",
    "    vehicle_images_list.append(rgbImage)\n",
    "    vehicle_images_list.append(flippedImage)\n",
    "    \n",
    "print(\"Reading of vehicle images done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading of non vehicle images done...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading non car images\n",
    "no_vehicle_image_arr1 = glob.glob('/media/rd_square/Important/Image_processing/car_data_with_neg/non-vehicles/*/*.png')\n",
    "no_vehicle_image_arr2 = glob.glob('/media/rd_square/Important/Image_processing/haar_cascade_classifier/neg/*.jpg')\n",
    "\n",
    "# read images, flip images and append into list\n",
    "no_vehicle_images_list = []\n",
    "for imagePath in no_vehicle_image_arr1:\n",
    "    readImage = cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    rgbImage = cv2.resize(rgbImage, (64,64))\n",
    "    flippedImage = cv2.flip(rgbImage, flipCode=1) # flip horizontally\n",
    "    no_vehicle_images_list.append(rgbImage)\n",
    "    no_vehicle_images_list.append(flippedImage)\n",
    "    \n",
    "for imagePath in no_vehicle_image_arr2:\n",
    "    readImage = cv2.imread(imagePath)\n",
    "    rgbImage = cv2.cvtColor(readImage, cv2.COLOR_BGR2RGB)\n",
    "    rgbImage = cv2.resize(rgbImage, (64,64))\n",
    "    no_vehicle_images_list.append(rgbImage)\n",
    "    \n",
    "    \n",
    "print(\"Reading of non vehicle images done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Vehicles images:  1150\n",
      "No of non vehicle images:  1038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"No of Vehicles images: \", len(vehicle_images_list))\n",
    "print(\"No of non vehicle images: \", len(no_vehicle_images_list))\n",
    "#                       ENDING OF READING IMAGES FROM FILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#                       FEATURE EXTRACTION FROM IMAGES\n",
    "\n",
    "def GetFeaturesFromHog(image, orient, cellsPerBlock, pixelsPerCell,\n",
    "                       visualise=False, feature_vector_flag=True):\n",
    "    'To extract hog feature using hog() from YUV image'\n",
    "    hog_features = hog(image, orientations=orient,\n",
    "                                  pixels_per_cell=(pixelsPerCell, pixelsPerCell),\n",
    "                                  cells_per_block=(cellsPerBlock, cellsPerBlock),\n",
    "                                  visualize=visualise, feature_vector= feature_vector_flag, block_norm='L2-Hys')\n",
    "    return hog_features\n",
    "\n",
    "def ExtractFeatures(images, orientation=9, cellsPerBlock=2, pixelsPerCell=16, convertColorSpace=True):\n",
    "    'Arrangin features into horizontal stack extracted from hog()'\n",
    "    featureList = []\n",
    "    for image in images:\n",
    "        if(convertColorSpace):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "        feature1 = GetFeaturesFromHog(image[:,:,0], orientation, cellsPerBlock, pixelsPerCell)\n",
    "        feature2 = GetFeaturesFromHog(image[:,:,1], orientation, cellsPerBlock, pixelsPerCell)\n",
    "        feature3 = GetFeaturesFromHog(image[:,:,2], orientation, cellsPerBlock, pixelsPerCell)\n",
    "\n",
    "        # Arranging feature into horizontal stack for machine learning model training set\n",
    "        featureListing = np.hstack((feature1, feature2, feature3))\n",
    "        featureList.append(featureListing)\n",
    "    return featureList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features list is  (2188, 972)\n",
      "Shape of labels list is  (2188,)\n"
     ]
    }
   ],
   "source": [
    "vehicleFeatures = ExtractFeatures(vehicle_images_list)\n",
    "nonVehicleFeatures = ExtractFeatures(no_vehicle_images_list)\n",
    "\n",
    "''' Arranging features into vertical stack for training the machine learning model '''\n",
    "featureList = np.vstack([vehicleFeatures, nonVehicleFeatures])\n",
    "print(\"Shape of features list is \", featureList.shape)\n",
    "labelList = np.concatenate([np.ones(len(vehicleFeatures)), np.zeros(len(nonVehicleFeatures))])\n",
    "print(\"Shape of labels list is \", labelList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier Trained...\n",
      "Accuracy of trained svm classifier is  0.9908675799086758\n"
     ]
    }
   ],
   "source": [
    "#                       DATA PREPROCESSING\n",
    "\n",
    "# train and test split of data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(featureList, labelList, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "#                       TRAINING THE LINEAR SUPPORT VECTOR CLASSIFIER MODEL OF MACHINE LEARNING\n",
    "\n",
    "svcClassifier  = LinearSVC()\n",
    "svcClassifier.fit(X_train, Y_train) # training the classifier\n",
    "print(\"Support Vector Classifier Trained...\")\n",
    "print(\"Accuracy of trained svm classifier is \", svcClassifier.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video = cv2.VideoCapture('camera.mp4') # Capturing video from camera\n",
    "\n",
    "# Initialize the first frame in the video stream\n",
    "firstFrame = None\n",
    "count = 0\n",
    "\n",
    "# loop over the frames of the video\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "\n",
    "    # resize the frame, convert it to grayscale, and blur it\n",
    "    frame = imutils.resize(frame, width=500)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (31,31),0)\n",
    "\n",
    "    # if the first frame is None, initialize it\n",
    "    if count < 2:\n",
    "        firstFrame = gray\n",
    "        count += 1\n",
    "        continue\n",
    "\n",
    "    # Compute the absolute difference between the current frame and first frame\n",
    "    frameDelta = cv2.absdiff(firstFrame, gray)\n",
    "    thresh = cv2.threshold(frameDelta, 25, 255,cv2.THRESH_BINARY)[1] # use inverse binary threshold when needed\n",
    "\n",
    "    # dilate the thresholded image to fill in holes,\n",
    "    # then find contours on thresholded image\n",
    "    #thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    #thresh = cv2.dilate(thresh, cv2.getStructuringElement(), iterations=2)\n",
    "    \n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        if cv2.contourArea(c) < 200:\n",
    "            continue\n",
    "        (x,y,w,h) = cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h),(0,255,0),2)\n",
    "    \n",
    "    cv2.imshow('frame1', frame)\n",
    "    cv2.imshow('threshold', thresh)\n",
    "    k = cv2.waitKey(0)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
